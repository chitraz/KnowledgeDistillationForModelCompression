# Knowledge Distillation for Model Compression

This p

Various KD methods are explored to distill using primaly residual CNNs models on CIFAR-10/100 dataset. In addition a 


  - [Distiller.py](scripts/Distiller.py)
  - [Dataset.py](scripts/Dataset.py)
  - [KD_methods.py](scripts/KD_methods.py)
  - [Models](scripts/Models.py)
  - [Utils.py](scripts/Utils.py)




![image](https://github.com/chitraz/KnowledgeDistillationForModelCompression/assets/40371968/61d02532-9403-4e64-bdd8-ac4555614c64)



## Experimantal Results 

