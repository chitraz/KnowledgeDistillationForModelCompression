# Knowledge Distillation for Model Compression

This repo [FinalReport_Chitra.pdf](https://github.com/chitraz/KnowledgeDistillationForModelCompression/files/15062925/FinalReport_Chitra.pdf)

Various KD methods are explored to distill using primaly residual CNNs models on CIFAR-10/100 dataset. In addition a 

  - [Distiller.py](scripts/Distiller.py)
  - [Dataset.py](scripts/Dataset.py)
  - [KD_methods.py](scripts/KD_methods.py)
  - [Models](scripts/Models.py)
  - [Utils.py](scripts/Utils.py)





![image](https://github.com/chitraz/KnowledgeDistillationForModelCompression/assets/40371968/61d02532-9403-4e64-bdd8-ac4555614c64)



## Experimantal Results 




## Future works  

  - Expore KD alongside other compression techniques: Pruning/Quantisation 
  - KD for compressing Object Detection models
