# Knowledge Distillation for Model Compression

This p

Various KD methods are explored to distill using primaly residual CNNs models on CIFAR-10/100 dataset. In addition a 


[here](scripts/Distiller.py)




![image](https://github.com/chitraz/KnowledgeDistillationForModelCompression/assets/40371968/61d02532-9403-4e64-bdd8-ac4555614c64)



## Experimantal Results 

